{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process articles\n",
    "This notebook processes the text of the articles by forcing all words to lowercase and removing punctuation. It also ensures each article contains the word \"terror\", \"terrorism\", or \"terrorist\" at least once and removes those words to prevent clustering on them. It removes stopwords and other vocabulary associated with the New York Times website or otherwise deemed uninteresting to improve clustering. Finally, it tokenizes the articles by word and selects the adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:58:59.720820Z",
     "start_time": "2017-05-25T20:58:59.711016Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:52:38.205853Z",
     "start_time": "2017-05-25T20:52:37.880823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_alpha_num(corpus):\n",
    "    # convert to lower case\n",
    "    corpus = map(str.lower, corpus)\n",
    "    \n",
    "    # remove alpha-numerical words\n",
    "    corpus = map(lambda x: re.sub(r\"\"\"\\w*\\d\\w*\"\"\", '', x), corpus)\n",
    "    return list(corpus)\n",
    "\n",
    "\n",
    "def remove_punct(corpus):\n",
    "    # regular expression to remove punctuation\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    corpus = map(lambda x: punc_re.sub(' ', x), corpus)\n",
    "    return list(corpus)\n",
    "\n",
    "\n",
    "def pos_tag(corpus):        \n",
    "    return list(map(nltk.pos_tag, corpus))\n",
    "\n",
    "\n",
    "def word_tokens(corpus):\n",
    "    return list(map(word_tokenize, corpus))\n",
    "\n",
    "\n",
    "def remove_sws(corpus):\n",
    "    # stopword removal\n",
    "    stop_words = stopwords.words('english')\n",
    "    filter_fun = lambda x: list(filter(lambda x: x not in stop_words, x))\n",
    "\n",
    "    corpus = generalize_fun(corpus, filter_fun)\n",
    "    return list(corpus)\n",
    "\n",
    "\n",
    "def get_adj(doc):\n",
    "    out = [tag for tag in doc if tag[1][0] =='J' or tag[1][0] == 'R']\n",
    "    return out\n",
    "\n",
    "\n",
    "def generalize_fun(corpus, lambda_fun):\n",
    "    # must handle a list of lists (tokenized docs) and also a simple list\n",
    "    \n",
    "    if isinstance(corpus[0], list):\n",
    "        # list of lists\n",
    "        corpus = map(lambda_fun, corpus)\n",
    "    else:\n",
    "        # single list\n",
    "        corpus = lambda_fun(corpus)\n",
    "        \n",
    "    return list(corpus)\n",
    "\n",
    "\n",
    "def remove_nyt_words(articles, words = ['story',\n",
    "                                        'united',\n",
    "                                        'states',\n",
    "                                        'american',\n",
    "                                        'america',\n",
    "                                        'mr',\n",
    "                                        'ms',\n",
    "                                        'said',\n",
    "                                        'main', \n",
    "                                        'continue', \n",
    "                                        'reading', \n",
    "                                        'advertisement',\n",
    "                                        'new',\n",
    "                                        'york',\n",
    "                                        'times',\n",
    "                                        'newsletter',\n",
    "                                        'sletter',\n",
    "                                        'sign up',\n",
    "                                        \"please verify you're not a robot by clicking the box\",\n",
    "                                        'invalid email address',\n",
    "                                        \"please re-enter\",\n",
    "                                        \"you must select a\",\n",
    "                                        \"to subscribe to\",\n",
    "                                        \"receive occasional updates and special offers\",\n",
    "                                        \"products and services\",\n",
    "                                        \"thank you for subscribing\",\n",
    "                                        \"an error has occurred\",\n",
    "                                        \"please try again later\",\n",
    "                                        \"view all new york\"]):\n",
    "    out = []\n",
    "    for article in articles:\n",
    "        for word in words:\n",
    "            article = article.lower().replace(word,'')\n",
    "        out.append(article)  \n",
    "    \n",
    "    return out \n",
    "\n",
    "\n",
    "def filter_for_terror_words(articles, words = ['terror','terrorism','terrorist']):\n",
    "    \n",
    "    out = []\n",
    "    for article in articles:\n",
    "        flag = 0\n",
    "        for word in words:\n",
    "            if word in article:\n",
    "                article = article.lower().replace(word,'')\n",
    "                flag = 1\n",
    "        if flag == 1:       \n",
    "            out.append(article)  \n",
    "    \n",
    "    return out \n",
    "\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    corpus = filter_for_terror_words(corpus)\n",
    "    corpus = remove_nyt_words(corpus)\n",
    "    corpus = lower_alpha_num(corpus)\n",
    "    corpus = remove_punct(corpus)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import nltk vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T21:16:54.278594Z",
     "start_time": "2017-05-25T21:16:54.248680Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_path = '/Users/Chris/ds/metis/notebooks/code_along/data/nltk_data'\n",
    "nltk.data.path.insert(0, nltk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:52:45.251453Z",
     "start_time": "2017-05-25T20:52:45.206986Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paths = ! ls ../data/pickle_jar/*_text.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:52:46.589660Z",
     "start_time": "2017-05-25T20:52:46.070929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yearly_corpus = {}\n",
    "\n",
    "for path in paths:\n",
    "    year = path.split('/')[3].rstrip('_text.p')\n",
    "    article_set = pickle.load(open(path,'rb'))\n",
    "    yearly_corpus[year] = list(article_set.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:53:20.734883Z",
     "start_time": "2017-05-25T20:52:57.390929Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_yearly_corpus = {k: clean_corpus(v) for k,v in yearly_corpus.items()}\n",
    "pickle.dump(clean_yearly_corpus,open('../data/pickle_jar/clean_yearly_corpus_1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:53:24.930759Z",
     "start_time": "2017-05-25T20:53:24.896611Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'federal investigators  yesterday that several men still at large were believed to be part of the ist group seized this week on the brink of carrying out a spectacular islamic radical plot to assassina'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of cleaned article\n",
    "clean_yearly_corpus['1993'][0][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:54:22.059643Z",
     "start_time": "2017-05-25T20:53:30.868809Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_yearly_corpus = {k: word_tokens(v) for k,v in clean_yearly_corpus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:54:57.344761Z",
     "start_time": "2017-05-25T20:54:30.783054Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_yearly_corpus = {k: remove_sws(v) for k,v in token_yearly_corpus.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T20:55:36.895320Z",
     "start_time": "2017-05-25T20:55:36.889433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['federal',\n",
       " 'investigators',\n",
       " 'yesterday',\n",
       " 'several',\n",
       " 'men',\n",
       " 'still',\n",
       " 'large',\n",
       " 'believed',\n",
       " 'part',\n",
       " 'ist']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tokenized article\n",
    "token_yearly_corpus['1993'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts-of-speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T21:25:53.523329Z",
     "start_time": "2017-05-25T21:17:04.761925Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_tagged = {k: pos_tag(v) for k,v in token_yearly_corpus.items()}\n",
    "pickle.dump(corpus_tagged,open('../data/pickle_jar/tagged_corpus_1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T21:26:18.536681Z",
     "start_time": "2017-05-25T21:26:18.478488Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('federal', 'JJ'),\n",
       " ('investigators', 'NNS'),\n",
       " ('yesterday', 'NN'),\n",
       " ('several', 'JJ'),\n",
       " ('men', 'NNS')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tagged tokens\n",
    "corpus_tagged['1993'][0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for ajectives and adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T21:26:39.433859Z",
     "start_time": "2017-05-25T21:26:35.565996Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_ad = {k: list(map(get_adj,v)) for k,v in corpus_tagged.items()}\n",
    "pickle.dump(corpus_ad,open('../data/pickle_jar/corpus_ads_1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-25T21:28:56.342920Z",
     "start_time": "2017-05-25T21:28:56.326637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('federal', 'JJ'),\n",
       " ('several', 'JJ'),\n",
       " ('still', 'RB'),\n",
       " ('large', 'JJ'),\n",
       " ('spectacular', 'JJ')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of tagged adjectives and adverbs\n",
    "corpus_ad['1993'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
